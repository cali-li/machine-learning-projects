---
title: "HW5"
author: "Wenjing Li"
date: "3/29/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 2

The dataset is from **MNIST database**. It contains handwritten digits. We will use the training dataset to train the model and get models using **SVM** and **Neural Network** methods respectively.

```{r,echo=FALSE, warning=FALSE}
#load data
load("C:/Users/wenji/Downloads/STATS 503/HW5/mnist.Rdata")
#loads package
suppressMessages(suppressWarnings(library(e1071)))
suppressMessages(suppressWarnings(library(keras)))
suppressMessages(suppressWarnings(library(tensorflow)))
suppressMessages(suppressWarnings(library(dplyr)))
suppressMessages(suppressWarnings(library(tidyverse)))
suppressMessages(suppressWarnings(library(tidyr)))
suppressMessages(suppressWarnings(library(ggplot2)))
```

Get some basic description of the dataset.

```{r}
par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:25) { 
  train <- x_train[i, , ]
  img <- t(apply(train, 2, rev)) 
  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
        main = paste(y_train[i]))
}
```

Preprocess the data.

```{r}
x_train <- x_train / 255
x_test <- x_test / 255

x_train.1 <- matrix(x_train, dim(x_train)[1], prod(dim(x_train)[2:3]))
x_test.1 <- matrix(x_test, dim(x_test)[1], prod(dim(x_test)[2:3]))

train_labels = as.factor(y_train)
test_labels = as.factor(y_test)

train_dat = data.frame(train_labels[1:1000], x_train.1[1:1000,])
colnames(train_dat)[1] = "labels"

test_dat = data.frame(test_labels, x_test.1)
colnames(test_dat)[1] = "labels"
```

Train the SVM model and get the best model as: gamma = 0.1, cost = 0.1, degree = 1, kernel = polynomial.

```{r,warning=FALSE}
library(e1071)
set.seed(123)
tc = tune.control(cross=5)
tune.out =tune(svm, labels~., data=train_dat,ranges=list(gamma = c(0.1,0.5),degree=c(1,2,3),cost=c(0.1,1), kernel=c("polynomial", "radial")), tunecontrol = tc)
#tune.out =tune(svm, labels~., data=train_dat,  kernel="radial")
summary(tune.out)
```

```{r}
test_pred = predict(tune.out$best.model, newdata = test_dat)
table(test_pred, test_dat$labels)
(acc = 1-sum((test_pred != test_labels)) / dim(test_dat)[1])
```

So the accuracy for the best svm selected form the above informmation is:

### 2(b)

Build MLP (**Multi Layer Perception**)

```{r}
load("C:/Users/wenji/Downloads/STATS 503/HW5/mnist.Rdata")
x_train <- x_train / 255
x_test <- x_test / 255
model_fashion <- keras_model_sequential()
model_fashion %>%
  layer_flatten(input_shape = c(28, 28)) %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')
model_fashion %>% compile(
  optimizer = 'adam', 
  loss = 'sparse_categorical_crossentropy',
  metrics = c('accuracy')
)
MLP.history = model_fashion %>% 
  fit(x_train, y_train, epochs = 30, 
      validation_split = 0.2, batch_size = 32)
plot(MLP.history) + theme_minimal()
```

And we can get the test information as below.

```{r}
score.mlp <- model_fashion %>% evaluate(x_test, y_test)
cat('Test accuracy:', score.mlp$acc, "\n")
```

Build the structure of the **CNN** model:

At first, create the structure of the CNN model.

```{r}
model_fashion.cnn <- keras_model_sequential()
#configuring the Model
model_fashion.cnn %>%  
#defining a 2-D convolution layer
  layer_conv_2d(filter=32,kernel_size=c(3,3),
                padding="same",input_shape=c(28,28,1)) %>%
  layer_activation("relu") %>%
  layer_max_pooling_2d(pool_size=c(2,2)) %>% 
#another 2-D convolution layer
  layer_conv_2d(filter=32 ,kernel_size=c(3,3)) %>%  
  layer_activation("relu") %>%
#Defining a Pooling layer which reduces the dimentions of the 
#features map and reduces the computational complexity of the model
  layer_max_pooling_2d(pool_size=c(2,2)) %>%  
#dropout layer to avoid overfitting
  layer_dropout(0.25) %>%
#flatten the input  
  layer_flatten() %>%  
  layer_dense(64) %>%  
  layer_activation("relu") %>%  
  layer_dropout(0.5) %>%  
#output layer-10 classes-10 units  
  layer_dense(10) %>%  
#applying softmax nonlinear activation function to the output layer
#to calculate cross-entropy  
  layer_activation("softmax")
```

Train the model and generate a history plot.

```{r}
train_images.cnn = array(x_train, dim = c(dim(x_train)[1], 
                                          dim(x_train)[2], dim(x_train)[3], 1))
test_images.cnn = array(x_test, dim = c(dim(x_test)[1], 
                                        dim(x_test)[2], dim(x_test)[3], 1))
model_fashion.cnn %>% compile(
  optimizer = 'adam', 
  loss = 'sparse_categorical_crossentropy',
  metrics = c('accuracy')
)
CNN.history = model_fashion.cnn %>% 
  fit(train_images.cnn, y_train, epochs = 30, 
      validation_split = 0.2, batch_size = 32)
plot(CNN.history) + theme_minimal()
```

And then, we can get test accuracy as below.

```{r}
score.cnn <- model_fashion.cnn %>% evaluate(test_images.cnn, y_test)
cat('Test accuracy:', score.cnn$acc, "\n")
```

To warp up, we can get a table.

```{r}
table_error = c(acc,
                score.mlp$acc,
                score.cnn$acc)
dim(table_error) = c(1,3)
colnames(table_error) = c("SVM", "NN(MLP)", "NN(CNN)")
rownames(table_error) = c("Testing Accuracy")
cap = paste("*Training Accuracy for 3 Methods*")
knitr::kable(table_error, caption = cap)
```
